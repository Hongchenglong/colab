{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T11:19:35.220902Z",
     "start_time": "2021-12-21T11:19:35.154904Z"
    }
   },
   "outputs": [],
   "source": [
    "#Author: Matthew Wicker\n",
    "# Impliments the BayesByBackprop optimizer for BayesKeras\n",
    "\n",
    "import os\n",
    "import math\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "from BayesKeras.optimizers import optimizer \n",
    "from BayesKeras.optimizers import losses\n",
    "from BayesKeras import analyzers\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# A dumb mistake on my part which needs to be factored out\n",
    "def softplus(x):\n",
    "     return tf.math.softplus(x)\n",
    "\n",
    "class BayesByBackprop(optimizer.Optimizer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    # I set default params for each sub-optimizer but none for the super class for\n",
    "    # pretty obvious reasons\n",
    "    def compile(self, keras_model, loss_fn, batch_size=64, learning_rate=0.15, decay=0.0,\n",
    "                      epochs=10, prior_mean=-1, prior_var=-1, **kwargs):\n",
    "        super().compile(keras_model, loss_fn, batch_size, learning_rate, decay,\n",
    "                      epochs, prior_mean, prior_var, **kwargs)\n",
    "\n",
    "\n",
    "        # Now we get into the BayesByBackprop specific enrichments to the class\n",
    "        # 计算后验方差\n",
    "        # Post process our variances to all be stds:\n",
    "        for i in range(len(self.posterior_var)):\n",
    "            self.posterior_var[i] = tf.math.log(tf.math.exp(self.posterior_var[i])-1)\n",
    "        self.kl_weight = kwargs.get('kl_weight', 1.0)              \n",
    "        self.kl_component = tf.keras.metrics.Mean(name=\"kl_comp\")  \n",
    "        print(\"BayesKeras: Using passed loss_fn as the data likelihood in the KL loss\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def step(self, features, labels, lrate):\n",
    "        \"\"\"\n",
    "        Initial sampling for BBB\n",
    "        \"\"\"\n",
    "        init_weights = []; noise_used = []\n",
    "        for i in range(len(self.posterior_mean)):\n",
    "            noise = tf.random.normal(shape=self.posterior_var[i].shape, \n",
    "                                     mean=tf.zeros(self.posterior_var[i].shape), stddev=1.0)\n",
    "            var_add = tf.multiply(softplus(self.posterior_var[i]), noise)\n",
    "            #var_add = tf.multiply(self.posterior_mean[i], noise)\n",
    "            w = tf.math.add(self.posterior_mean[i], var_add)\n",
    "            noise_used.append(noise)\n",
    "            init_weights.append(w)\n",
    "        self.model.set_weights(init_weights)\n",
    "\n",
    "        # Define the GradientTape context\n",
    "        with tf.GradientTape(persistent=True) as tape:   # Below we add an extra variable for IBP\n",
    "            tape.watch(self.posterior_mean) \n",
    "            tape.watch(self.posterior_var); #tape.watch(init_weights)\n",
    "            predictions = self.model(features)\n",
    "\n",
    "            if(self.robust_train == 0):\n",
    "                worst_case = predictions # cheap hack lol\n",
    "                loss, kl_comp = losses.KL_Loss(labels, predictions, self.model.trainable_variables,\n",
    "                                               self.prior_mean, self.prior_var, \n",
    "                                               self.posterior_mean, self.posterior_var, \n",
    "                                               self.loss_func, self.kl_weight)\n",
    "            elif(int(self.robust_train) == 1):\n",
    "                # Get the probabilities\n",
    "                logit_l, logit_u = analyzers.IBP(self, features, self.model.trainable_variables, eps=self.epsilon)\n",
    "                #!*! TODO: Undo the hardcoding of depth in this function\n",
    "                v1 = tf.one_hot(labels, depth=10)\n",
    "                v2 = 1 - tf.one_hot(labels, depth=10)\n",
    "                worst_case = tf.math.add(tf.math.multiply(v2, logit_u), tf.math.multiply(v1, logit_l))\n",
    "\n",
    "                # Now we have the worst case softmax probabilities\n",
    "                worst_case = self.model.layers[-1].activation(worst_case)\n",
    "                # Calculate the loss\n",
    "                loss, kl_comp = losses.robust_KL_Loss(labels, predictions, self.model.trainable_variables,\n",
    "\t       \t\t                              self.prior_mean, self.prior_var, \n",
    "                                                      self.posterior_mean, self.posterior_var, \n",
    "                                                      self.loss_func, self.kl_weight, \n",
    "                                                      worst_case, self.robust_lambda)\n",
    "            \n",
    "            elif(int(self.robust_train) == 2):\n",
    "                features_adv = analyzers.PGD(self, features, self.attack_loss, eps=self.epsilon, num_models=-1)\n",
    "                # Get the probabilities\n",
    "                worst_case = self.model(features_adv)\n",
    "                #print(predictions[0], worst_case[0])\n",
    "                # Calculate the loss\n",
    "                loss, kl_comp = losses.robust_KL_Loss(labels, predictions, self.model.trainable_variables,\n",
    "                                                      self.prior_mean, self.prior_var,\n",
    "                                                      self.posterior_mean, self.posterior_var,\n",
    "                                                      self.loss_func, self.kl_weight,\n",
    "                                                      worst_case, self.robust_lambda)\n",
    "            # !*! Now this is the broken branch\n",
    "            elif(int(self.robust_train) == 3):\n",
    "                sys.exit(0)\n",
    "                logit_l, logit_u = analyzers.IBP(self, features, self.model.trainable_variables, eps=self.epsilon)\n",
    "                v1 = tf.one_hot(labels, depth=10)\n",
    "                v2 = 1 - tf.one_hot(labels, depth=10)\n",
    "                worst_case = tf.math.add(tf.math.multiply(v2, logit_u), tf.math.multiply(v1, logit_l))\n",
    "                worst_case = self.model.layers[-1].activation(worst_case)\n",
    "                one_hot_cls = tf.one_hot(labels, depth=10)\n",
    "                output = tf.math.reduce_max((self.robust_lambda*(predictions*one_hot_cls))  + ((1-self.robust_lambda)*(worst_case*one_hot_cls)), axis=1)\n",
    "                loss = self.loss_func(labels, predictions)\n",
    "\n",
    "            elif(int(self.robust_train) == 5):\n",
    "                output = tf.zeros(predictions.shape)\n",
    "                self.epsilon = max(0.0001, self.epsilon)\n",
    "                self.eps_dist = tfp.distributions.Exponential(1.0/self.epsilon)\n",
    "                for _mc_ in range(self.loss_monte_carlo):\n",
    "                    #eps = tfp.random.rayleigh([1], scale=self.epsilon)\n",
    "                    eps = self.eps_dist.sample()\n",
    "                    logit_l, logit_u = analyzers.IBP(self, features, self.model.trainable_variables, eps=eps)\n",
    "                    v1 = tf.one_hot(labels, depth=10)\n",
    "                    v2 = 1 - tf.one_hot(labels, depth=10)\n",
    "                    v1 = tf.squeeze(v1); v2 = tf.squeeze(v2)\n",
    "                    worst_case = tf.math.add(tf.math.multiply(v2, logit_u), tf.math.multiply(v1, logit_l))\n",
    "                    worst_case = self.model.layers[-1].activation(worst_case)\n",
    "                    one_hot_cls = tf.one_hot(labels, depth=10)\n",
    "                    output += (1.0/self.loss_monte_carlo) * worst_case\n",
    "\n",
    "                loss, kl_comp = losses.KL_Loss(labels, output, self.model.trainable_variables,\n",
    "                                               self.prior_mean, self.prior_var, \n",
    "                                               self.posterior_mean, self.posterior_var, \n",
    "                                               self.loss_func, self.kl_weight)\n",
    "\n",
    "            elif(int(self.robust_train) == 6):\n",
    "                output = tf.zeros(predictions.shape)\n",
    "                self.epsilon = max(0.0001, self.epsilon)\n",
    "                self.eps_dist = tfp.distributions.Exponential(1.0/self.epsilon)\n",
    "                for _mc_ in range(self.loss_monte_carlo):\n",
    "                    #eps = tfp.random.rayleigh([1], scale=self.epsilon)\n",
    "                    eps = self.eps_dist.sample()\n",
    "                    features_adv = analyzers.FGSM(self, features, self.attack_loss, eps=self.epsilon, num_models=-1)\n",
    "                    worst_case = self.model(features_adv)\n",
    "                    output += (1.0/self.loss_monte_carlo) * worst_case\n",
    "                loss, kl_comp = losses.KL_Loss(labels, output, self.model.trainable_variables,\n",
    "                                               self.prior_mean, self.prior_var, \n",
    "                                               self.posterior_mean, self.posterior_var, \n",
    "                                               self.loss_func, self.kl_weight)\n",
    "        # Get the gradients\n",
    "        weight_gradient = tape.gradient(loss, self.model.trainable_variables)\n",
    "        mean_gradient = tape.gradient(loss, self.posterior_mean)\n",
    "        var_gradient = tape.gradient(loss, self.posterior_var)\n",
    "        #init_gradient = tape.gradient(loss, init_weights)\n",
    "        \n",
    "        posti_mean_grad = []\n",
    "        posti_var_grad = []\n",
    "        # !*! - Make the weight and init gradients the same variable and retest\n",
    "        for i in range(len(mean_gradient)):\n",
    "            #weight_gradient[i] = tf.math.add(weight_gradient[i], init_gradient[i])\n",
    "            weight_gradient[i] = tf.cast(weight_gradient[i], 'float32')\n",
    "            mean_gradient[i] = tf.cast(mean_gradient[i], 'float32')\n",
    "            f = tf.math.add(weight_gradient[i], mean_gradient[i])\n",
    "            posti_mean_grad.append(f)\n",
    "            v = tf.math.divide(noise_used[i], 1+tf.math.exp(tf.math.multiply(self.posterior_var[i], -1)))\n",
    "            v = tf.math.multiply(v, weight_gradient[i])\n",
    "            v = tf.math.add(v, var_gradient[i])\n",
    "            posti_var_grad.append(v)\n",
    "        #gradients = posti_mean_grad\n",
    "\n",
    "        # APPLICATION OF WEIGHTS\n",
    "        new_posti_var = []; new_posti_mean = []\n",
    "        for i in range(len(mean_gradient)):\n",
    "            pdv = tf.math.multiply(posti_var_grad[i], lrate)\n",
    "            pdm = tf.math.multiply(posti_mean_grad[i], lrate)\n",
    "            v = tf.math.subtract(self.posterior_var[i], pdv)\n",
    "            m = tf.math.subtract(self.posterior_mean[i], pdm)\n",
    "            new_posti_var.append(v)\n",
    "            new_posti_mean.append(m)\n",
    "\n",
    "        self.train_loss(loss)\n",
    "        self.train_metric(labels, predictions)\n",
    "        #self.train_rob(labels, worst_case)\n",
    "        self.kl_component(kl_comp)\n",
    "        self.posterior_mean = new_posti_mean\n",
    "        self.posterior_var = new_posti_var\n",
    "        return new_posti_mean, new_posti_var\n",
    "\n",
    "    \n",
    "    def train(self, X_train, y_train, X_test=None, y_test=None):\n",
    "        super().train(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    def sample(self):\n",
    "        sampled_weights = []\n",
    "        for i in range(len(self.posterior_mean)):\n",
    "            sampled_weights.append(np.random.normal(loc=self.posterior_mean[i],\n",
    "                                                    scale=softplus(self.posterior_var[i])))\n",
    "        return sampled_weights\n",
    "\n",
    "\n",
    "    def save(self, path):\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        var = []\n",
    "        for i in range(len(self.posterior_var)):\n",
    "            var.append(softplus(self.posterior_var[i]))\n",
    "        np.save(path+\"/mean\", np.asarray(self.posterior_mean))\n",
    "        np.save(path+\"/var\", np.asarray(var))\n",
    "        self.model.save(path+'/model.h5')\n",
    "        model_json = self.model.to_json()\n",
    "        with open(path+\"/arch.json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T09:47:50.598987Z",
     "start_time": "2021-12-21T09:47:39.093477Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\T470\\Anaconda3\\envs\\CertifiableBayesianInference\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "path = Path(os.getcwd())\n",
    "sys.path.append(str(path.parent))\n",
    "\n",
    "import BayesKeras\n",
    "import BayesKeras.optimizers as optimizers\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T09:47:50.630985Z",
     "start_time": "2021-12-21T09:47:50.619988Z"
    }
   },
   "outputs": [],
   "source": [
    "dict = {'eps': 0.11, 'lam': 0.25, 'rob': 1, 'opt': 'BBB', 'gpu': '-1'}\n",
    "\n",
    "# epsilon:  the strength of the adversary, 在测试点周围多大的区域里考虑它的鲁棒性\n",
    "eps = dict['eps']\n",
    "# lamada: the probability of seeing the clean data (lambda = 1.0 means no adversarial data considered, lambda = 0.0 means only adversarial data considered)\n",
    "lam = dict['lam'] \n",
    "# robustness mode, 0:标准 1:IBP 2:PGD\n",
    "rob = dict['rob'] \n",
    "# the variational inference method\n",
    "optim = dict['opt']\n",
    "gpu = dict['gpu']\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T09:49:11.954276Z",
     "start_time": "2021-12-21T09:49:11.189147Z"
    }
   },
   "outputs": [],
   "source": [
    "# 划分训练集，测试集\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train/255.\n",
    "X_test = X_test/255.\n",
    "X_train = X_train.astype(\"float32\").reshape(-1, 28*28)\n",
    "X_test = X_test.astype(\"float32\").reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T09:49:29.947206Z",
     "start_time": "2021-12-21T09:49:27.675498Z"
    }
   },
   "outputs": [],
   "source": [
    "# 构建Sequential模型\n",
    "model = Sequential()\n",
    "# 构建Dense隐藏层并添加到模型中\n",
    "# 添加具有512个神经元的Dense隐藏层，使用relu激活函数\n",
    "model.add(Dense(512, activation=\"relu\", input_shape=(None, 28*28)))\n",
    "# 添加具有10个神经元的Dense隐藏层，使用softmax激活函数\n",
    "model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T11:09:46.358208Z",
     "start_time": "2021-12-21T11:09:46.339217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This optimizer does not have a default compilation method. Please make sure to call the correct .compile method before use.\n"
     ]
    }
   ],
   "source": [
    "inf = 10\n",
    "learning_rate = 0.45; decay=0.0\n",
    "opt = optimizers.BayesByBackprop()\n",
    "\n",
    "if (rob == 0):\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "elif (rob != 0):\n",
    "    loss = BayesKeras.optimizers.losses.robust_crossentropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T11:09:47.447210Z",
     "start_time": "2021-12-21T11:09:47.268212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BayesKeras: Using implicit prior\n",
      "(784, 512) 0.03571428571428571\n",
      "(512, 10) 0.04419417382415922\n",
      "BayesKeras: Using implicit prior\n",
      "(784, 512) 0.03571428571428571\n",
      "(512, 10) 0.04419417382415922\n",
      "BayesKeras: Detected robust training at compilation. Please ensure you have selected a robust-compatible loss\n",
      "BayesKeras: Using passed loss_fn as the data likelihood in the KL loss\n"
     ]
    }
   ],
   "source": [
    "bayes_model = opt.compile(model, loss_fn=loss, learning_rate=learning_rate, epochs=1, # epochs=20, \n",
    "                          batch_size=128, linear_schedule=True,\n",
    "                          decay=decay, robust_train=rob, c=inf,\n",
    "                          burn_in=3, steps=25, b_steps=20, epsilon=eps, rob_lam=lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CertifiableBayesianInference",
   "language": "python",
   "name": "certifiablebayesianinference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
