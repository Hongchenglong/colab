{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-23T15:28:36.155299Z",
     "start_time": "2021-12-23T15:28:35.843575Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path(os.getcwd())\n",
    "sys.path.append(str(path.parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-23T16:05:20.104467Z",
     "start_time": "2021-12-23T16:05:19.870289Z"
    },
    "code_folding": [
     308,
     317,
     342
    ]
   },
   "outputs": [],
   "source": [
    "# Impliments the BayesByBackprop optimizer for BayesKeras\n",
    "\n",
    "import os\n",
    "import math\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "from BayesKeras.optimizers import optimizer\n",
    "from BayesKeras.optimizers import losses\n",
    "from BayesKeras import analyzers\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# A dumb mistake on my part which needs to be factored out\n",
    "\n",
    "\n",
    "def softplus(x):\n",
    "    return tf.math.softplus(x)\n",
    "\n",
    "\n",
    "class BayesByBackprop():\n",
    "    def __init__(self):\n",
    "        print(\"This optimizer does not have a default compilation method. Please make sure to call the correct .compile method before use.\")\n",
    "\n",
    "    # I set default params for each sub-optimizer but none for the super class for pretty obvious reasons\n",
    "    def compile(self, keras_model, loss_fn, batch_size=64, learning_rate=0.15, decay=0.0,\n",
    "                epochs=10, prior_mean=-1, prior_var=-1, **kwargs):\n",
    "        ###############################super().compile()###############################\n",
    "        self.model = keras_model\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.epochs = epochs\n",
    "        self.loss_func = loss_fn\n",
    "\n",
    "        self.log_dir = kwargs.get('log_file', '/tmp/BayesKeras.log')\n",
    "\n",
    "        self.det = kwargs.get('deterministic', False)\n",
    "        self.inflate_prior = kwargs.get('inflate_prior', 1)\n",
    "        self.input_noise = kwargs.get('input_noise', 0.0)\n",
    "        # 先验、后验都调用prior_generator\n",
    "        print(\"===============================进入prior_generator===============================\")\n",
    "        self.prior_mean, self.prior_var = self.prior_generator(\n",
    "            prior_mean, prior_var)\n",
    "        self.posterior_mean, self.posterior_var = self.prior_generator(\n",
    "            prior_mean, prior_var)\n",
    "        print(\"===============================退出prior_generator===============================\")\n",
    "\n",
    "        self.train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "        self.valid_loss = tf.keras.metrics.Mean(name=\"valid_loss\")\n",
    "\n",
    "        # Right now I only have one accessory metric. Will come back and add many later.\n",
    "        self.train_metric = kwargs.get(\n",
    "            'metric', tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_acc\"))\n",
    "        self.valid_metric = kwargs.get(\n",
    "            'metric', tf.keras.metrics.SparseCategoricalAccuracy(name=\"valid_acc\"))\n",
    "        self.extra_metric = kwargs.get(\n",
    "            'metric', tf.keras.metrics.SparseCategoricalAccuracy(name=\"extra_acc\"))\n",
    "\n",
    "        self.robust_train = kwargs.get('robust_train', 0)\n",
    "        if(self.robust_train != 0):\n",
    "            print(\"BayesKeras: Detected robust training at compilation. Please ensure you have selected a robust-compatible loss\")\n",
    "            self.epochs += 1\n",
    "        self.epsilon = kwargs.get('epsilon', 0.1)\n",
    "        self.robust_lambda = kwargs.get('rob_lam', 0.5)\n",
    "        self.robust_linear = kwargs.get('linear_schedule', True)\n",
    "\n",
    "        self.attack_loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "        self.loss_monte_carlo = kwargs.get('loss_mc', 2)\n",
    "        self.eps_dist = tfp.distributions.Exponential(\n",
    "            rate=1.0/float(self.epsilon))\n",
    "\n",
    "        self.acc_log = []\n",
    "        self.rob_log = []\n",
    "        self.loss_log = []\n",
    "        ###############################super().compile()###############################\n",
    "\n",
    "        # Now we get into the BayesByBackprop specific enrichments to the class\n",
    "        # Post process our variances to all be stds:\n",
    "        print(\"计算后验方差\")\n",
    "        for i in range(len(self.posterior_var)):\n",
    "            self.posterior_var[i] = tf.math.log(\n",
    "                tf.math.exp(self.posterior_var[i])-1)\n",
    "        self.kl_weight = kwargs.get('kl_weight', 1.0)\n",
    "        self.kl_component = tf.keras.metrics.Mean(name=\"kl_comp\")\n",
    "        print(\"BayesKeras: Using passed loss_fn as the data likelihood in the KL loss\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def train(self, X_train, y_train, X_test=None, y_test=None):\n",
    "        ###############################super().train()###############################\n",
    "        self.N = len(X_train)\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(\n",
    "            100).batch(self.batch_size)  # from_tensor_slices: 切分传入Tensor的第一个维度，生成相应的dataset\n",
    "        test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "            (X_test, y_test)).batch(self.batch_size)\n",
    "\n",
    "        if(self.robust_linear):\n",
    "            self.max_eps = self.epsilon\n",
    "            self.epsilon = 0.0\n",
    "            self.max_robust_lambda = self.robust_lambda\n",
    "\n",
    "        lr = self.learning_rate\n",
    "        decay = self.decay\n",
    "        for epoch in range(self.epochs):\n",
    "            lrate = self.learning_rate * (1 / (1 + self.decay * epoch))\n",
    "\n",
    "            # Run the model through train and test sets respectively\n",
    "            #########################################################################\n",
    "            for (features, labels) in tqdm(train_ds):\n",
    "                features += np.random.normal(loc=0.0, scale=self.input_noise, size=features.shape)\n",
    "                self.posterior, self.posterior_var = self.step(\n",
    "                    features, labels, lrate)\n",
    "            for test_features, test_labels in test_ds:\n",
    "                self.model_validate(test_features, test_labels)\n",
    "            #########################################################################\n",
    "                \n",
    "            # Grab the results\n",
    "            (loss, acc) = self.train_loss.result(), self.train_metric.result()\n",
    "            (val_loss, val_acc) = self.valid_loss.result(), self.valid_metric.result()\n",
    "            self.logging(loss, acc, val_loss, val_acc, epoch)\n",
    "\n",
    "            # Clear the current state of the metrics\n",
    "            self.train_loss.reset_states(), self.train_metric.reset_states()\n",
    "            self.valid_loss.reset_states(), self.valid_metric.reset_states()\n",
    "            self.extra_metric.reset_states()\n",
    "\n",
    "            if(self.robust_linear):\n",
    "                self.epsilon += self.max_eps/self.epochs\n",
    "        ###############################super().train()###############################\n",
    "\n",
    "    def step(self, features, labels, lrate):\n",
    "        \"\"\"\n",
    "        Initial sampling for BBB\n",
    "        \"\"\"\n",
    "        init_weights = []\n",
    "        noise_used = []\n",
    "        for i in range(len(self.posterior_mean)):\n",
    "            noise = tf.random.normal(shape=self.posterior_var[i].shape,\n",
    "                                     mean=tf.zeros(self.posterior_var[i].shape), stddev=1.0)\n",
    "            var_add = tf.multiply(softplus(self.posterior_var[i]), noise)\n",
    "            #var_add = tf.multiply(self.posterior_mean[i], noise)\n",
    "            w = tf.math.add(self.posterior_mean[i], var_add)\n",
    "            noise_used.append(noise)\n",
    "            init_weights.append(w)\n",
    "        self.model.set_weights(init_weights)\n",
    "\n",
    "        # Define the GradientTape context\n",
    "        # Below we add an extra variable for IBP\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(self.posterior_mean)\n",
    "            tape.watch(self.posterior_var)  # tape.watch(init_weights)\n",
    "            predictions = self.model(features)\n",
    "\n",
    "            if(self.robust_train == 0):\n",
    "                worst_case = predictions  # cheap hack lol\n",
    "                loss, kl_comp = losses.KL_Loss(labels, predictions, self.model.trainable_variables,\n",
    "                                               self.prior_mean, self.prior_var,\n",
    "                                               self.posterior_mean, self.posterior_var,\n",
    "                                               self.loss_func, self.kl_weight)\n",
    "            elif(int(self.robust_train) == 1):\n",
    "                # Get the probabilities\n",
    "                logit_l, logit_u = analyzers.IBP(\n",
    "                    self, features, self.model.trainable_variables, eps=self.epsilon)\n",
    "                #!*! TODO: Undo the hardcoding of depth in this function\n",
    "                v1 = tf.one_hot(labels, depth=10)\n",
    "                v2 = 1 - tf.one_hot(labels, depth=10)\n",
    "                worst_case = tf.math.add(tf.math.multiply(\n",
    "                    v2, logit_u), tf.math.multiply(v1, logit_l))\n",
    "\n",
    "                # Now we have the worst case softmax probabilities\n",
    "                worst_case = self.model.layers[-1].activation(worst_case)\n",
    "                # Calculate the loss\n",
    "                loss, kl_comp = losses.robust_KL_Loss(labels, predictions, self.model.trainable_variables,\n",
    "                                                      self.prior_mean, self.prior_var,\n",
    "                                                      self.posterior_mean, self.posterior_var,\n",
    "                                                      self.loss_func, self.kl_weight,\n",
    "                                                      worst_case, self.robust_lambda)\n",
    "            elif(int(self.robust_train) == 2):\n",
    "                features_adv = analyzers.PGD(\n",
    "                    self, features, self.attack_loss, eps=self.epsilon, num_models=-1)\n",
    "                # Get the probabilities\n",
    "                worst_case = self.model(features_adv)\n",
    "                # print(predictions[0], worst_case[0])\n",
    "                # Calculate the loss\n",
    "                loss, kl_comp = losses.robust_KL_Loss(labels, predictions, self.model.trainable_variables,\n",
    "                                                      self.prior_mean, self.prior_var,\n",
    "                                                      self.posterior_mean, self.posterior_var,\n",
    "                                                      self.loss_func, self.kl_weight,\n",
    "                                                      worst_case, self.robust_lambda)\n",
    "        # Get the gradients\n",
    "        weight_gradient = tape.gradient(loss, self.model.trainable_variables)\n",
    "        mean_gradient = tape.gradient(loss, self.posterior_mean)\n",
    "        var_gradient = tape.gradient(loss, self.posterior_var)\n",
    "        #init_gradient = tape.gradient(loss, init_weights)\n",
    "\n",
    "        posti_mean_grad = []\n",
    "        posti_var_grad = []\n",
    "        # !*! - Make the weight and init gradients the same variable and retest\n",
    "        for i in range(len(mean_gradient)):\n",
    "            #weight_gradient[i] = tf.math.add(weight_gradient[i], init_gradient[i])\n",
    "            weight_gradient[i] = tf.cast(weight_gradient[i], 'float32')\n",
    "            mean_gradient[i] = tf.cast(mean_gradient[i], 'float32')\n",
    "            f = tf.math.add(weight_gradient[i], mean_gradient[i])\n",
    "            posti_mean_grad.append(f)\n",
    "            v = tf.math.divide(\n",
    "                noise_used[i], 1+tf.math.exp(tf.math.multiply(self.posterior_var[i], -1)))\n",
    "            v = tf.math.multiply(v, weight_gradient[i])\n",
    "            v = tf.math.add(v, var_gradient[i])\n",
    "            posti_var_grad.append(v)\n",
    "        #gradients = posti_mean_grad\n",
    "\n",
    "        # APPLICATION OF WEIGHTS\n",
    "        new_posti_var = []\n",
    "        new_posti_mean = []\n",
    "        for i in range(len(mean_gradient)):\n",
    "            pdv = tf.math.multiply(posti_var_grad[i], lrate)\n",
    "            pdm = tf.math.multiply(posti_mean_grad[i], lrate)\n",
    "            v = tf.math.subtract(self.posterior_var[i], pdv)\n",
    "            m = tf.math.subtract(self.posterior_mean[i], pdm)\n",
    "            new_posti_var.append(v)\n",
    "            new_posti_mean.append(m)\n",
    "\n",
    "        self.train_loss(loss)\n",
    "        self.train_metric(labels, predictions)\n",
    "        #self.train_rob(labels, worst_case)\n",
    "        self.kl_component(kl_comp)\n",
    "        self.posterior_mean = new_posti_mean\n",
    "        self.posterior_var = new_posti_var\n",
    "        return new_posti_mean, new_posti_var\n",
    "\n",
    "    def prior_generator(self, means, vars):\n",
    "        if(type(means) == int and type(vars) == int):\n",
    "            if(means < 0 or vars < 0):\n",
    "                model_mean, model_var = self._gen_implicit_prior()\n",
    "                # for i in range(len(model_var)):\n",
    "                #    model_var[i] = tf.math.log(tf.math.exp(self.model_var[i])-1)\n",
    "                return model_mean, model_var\n",
    "        if(type(means) == int or type(means) == float):\n",
    "            if(means == -1):\n",
    "                means = 0.0\n",
    "            mean_params = [means for i in range(len(self.model.weights))]\n",
    "            means = mean_params\n",
    "        if(type(vars) == int or type(vars) == float):\n",
    "            if(vars == -1):\n",
    "                vars = 0.0\n",
    "            var_params = [vars for i in range(len(self.model.weights))]\n",
    "            vars = var_params\n",
    "        model_mean = []\n",
    "        model_var = []\n",
    "        index = 0.0\n",
    "        for weight in self.model.weights:\n",
    "            param_index = math.floor(index/2.0)\n",
    "            mean_i = tf.math.multiply(\n",
    "                tf.ones(weight.shape), means[param_index])\n",
    "            vari_i = tf.math.multiply(tf.ones(weight.shape), vars[param_index])\n",
    "            model_mean.append(mean_i)\n",
    "            model_var.append(vari_i)\n",
    "            index += 1\n",
    "        return model_mean, model_var\n",
    "\n",
    "    \"\"\"\n",
    "    根据模型每一层的形状生成权重w和偏置b的先验均值分布\n",
    "    权重w：0\n",
    "    偏置b：std = math.sqrt(self.inflate_prior/(nin)) \n",
    "    \"\"\"\n",
    "    def _gen_implicit_prior(self):\n",
    "        print(\"BayesKeras: Using implicit prior\")\n",
    "        prior_mean = []\n",
    "        prior_var = []\n",
    "        for i in range(len(self.model.layers)):\n",
    "            try:\n",
    "                sha = self.model.layers[i].get_weights()[0].shape\n",
    "                b_sha = self.model.layers[i].get_weights()[1].shape\n",
    "                if(len(sha) > 2):\n",
    "                    nin = 1\n",
    "                    for i in range(len(sha)-1):\n",
    "                        nin *= sha[i]\n",
    "                else:\n",
    "                    nin = sha[0]\n",
    "                # 标准差\n",
    "                std = math.sqrt(self.inflate_prior/(nin))\n",
    "                print(sha, std)\n",
    "                mean_w = tf.zeros(sha)\n",
    "                var_w = tf.ones(sha) * std\n",
    "                mean_b = tf.zeros(b_sha)\n",
    "                var_b = tf.ones(b_sha) * std\n",
    "                # 分别加入权重w和偏置b的先验均值分布\n",
    "                prior_mean.append(mean_w)\n",
    "                prior_mean.append(mean_b)\n",
    "                prior_var.append(var_w)\n",
    "                prior_var.append(var_b)\n",
    "            except:\n",
    "                pass\n",
    "        return prior_mean, prior_var\n",
    "\n",
    "    def model_validate(self, features, labels):\n",
    "        # self.model.set_weights(self.sample())\n",
    "        predictions = self.model(features)\n",
    "        if (self.robust_train == 1):  # We only check with IBP if we need to\n",
    "            logit_l, logit_u = analyzers.IBP(self, features, self.model.get_weights(), self.epsilon)\n",
    "            # logit_l, logit_u = analyzers.IBP(self, features, self.model.trainable_variables, 0.0)\n",
    "            v1 = tf.one_hot(labels, depth=10)\n",
    "            v2 = 1 - tf.one_hot(labels, depth=10)\n",
    "            worst_case = tf.math.add(tf.math.multiply(v2, logit_u), tf.math.multiply(v1, logit_l))\n",
    "            worst_case = self.model.layers[-1].activation(worst_case)\n",
    "            v_loss = self.loss_func(labels, predictions, worst_case, self.robust_lambda)\n",
    "            self.extra_metric(labels, worst_case)\n",
    "        elif (self.robust_train == 2):\n",
    "            v_loss = self.loss_func(labels, predictions, predictions, self.robust_lambda)\n",
    "            worst_case = predictions\n",
    "        elif (self.robust_train == 3 or self.robust_train == 5):  # We only check with IBP if we need to\n",
    "            logit_l, logit_u = analyzers.IBP(self, features, self.model.get_weights(), self.epsilon)\n",
    "            # logit_l, logit_u = analyzers.IBP(self, features, self.model.trainable_variables, 0.0)\n",
    "            # print(logit_l.shape)\n",
    "            v1 = tf.squeeze(tf.one_hot(labels, depth=10))\n",
    "            v2 = tf.squeeze(1 - tf.one_hot(labels, depth=10))\n",
    "            # print(v1.shape, v2.shape)\n",
    "            worst_case = tf.math.add(tf.math.multiply(v2, logit_u), tf.math.multiply(v1, logit_l))\n",
    "            # print(worst_case.shape)\n",
    "            worst_case = self.model.layers[-1].activation(worst_case)\n",
    "            # print(worst_case.shape)\n",
    "            v_loss = self.loss_func(labels, worst_case)\n",
    "            self.extra_metric(labels, worst_case)\n",
    "        else:\n",
    "            v_loss = self.loss_func(labels, predictions)\n",
    "            worst_case = predictions\n",
    "        self.valid_metric(labels, predictions)\n",
    "        self.valid_loss(v_loss)\n",
    "        #self.valid_rob(labels, worst_case)\n",
    "\n",
    "    def logging(self, loss, acc, val_loss, val_acc, epoch):\n",
    "        # Local logging\n",
    "        if(self.robust_train == 0):\n",
    "            template = \"Epoch {}, loss: {:.3f}, acc: {:.3f}, val_loss: {:.3f}, val_acc: {:.3f}\"\n",
    "            print(template.format(epoch+1, loss, acc, val_loss, val_acc))\n",
    "        else:\n",
    "            rob = self.extra_metric.result()\n",
    "            template = \"Epoch {}, loss: {:.3f}, acc: {:.3f}, val_loss: {:.3f}, val_acc: {:.3f}, rob: {:.3f}, (eps = {:.6f})\"\n",
    "            print(template.format(epoch+1, loss, acc, val_loss, val_acc, rob, self.epsilon))\n",
    "        log_template = \"Epoch: {}, Train: [Loss: {:.3f}, Acc: {:.3f}], Test: [Loss: {:.3f}, Acc: {:.3f}]\"\n",
    "        logging.basicConfig(filename=self.log_dir, level=logging.DEBUG)\n",
    "        logging.info(log_template.format(epoch+1, loss, acc, val_loss, val_acc))\n",
    "\n",
    "    def save(self, path):\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        var = []\n",
    "        for i in range(len(self.posterior_var)):\n",
    "            var.append(softplus(self.posterior_var[i]))\n",
    "        np.save(path+\"/mean\", np.asarray(self.posterior_mean))\n",
    "        np.save(path+\"/var\", np.asarray(var))\n",
    "        self.model.save(path+'/model.h5')\n",
    "        model_json = self.model.to_json()\n",
    "        with open(path+\"/arch.json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        print(\"模型%s保存成功\" % path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-23T16:05:20.463463Z",
     "start_time": "2021-12-23T16:05:20.436463Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-23T16:05:21.346481Z",
     "start_time": "2021-12-23T16:05:20.827459Z"
    }
   },
   "outputs": [],
   "source": [
    "# 划分训练集，测试集\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train/255.\n",
    "X_test = X_test/255.\n",
    "# X_train = X_train.astype(\"float32\").reshape(-1, 28*28)\n",
    "# X_test = X_test.astype(\"float32\").reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-23T16:05:21.662459Z",
     "start_time": "2021-12-23T16:05:21.649460Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 构建Sequential模型\n",
    "# model = Sequential()\n",
    "# # 构建Dense隐藏层并添加到模型中\n",
    "# # 添加具有512个神经元的Dense隐藏层，使用relu激活函数\n",
    "# model.add(Dense(512, activation=\"relu\", input_shape=(None, 28*28)))\n",
    "# model.add(Dense(10, activation=\"softmax\"))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-23T16:05:22.202834Z",
     "start_time": "2021-12-23T16:05:22.022461Z"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-23T16:05:22.599610Z",
     "start_time": "2021-12-23T16:05:22.556603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_3 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 512)               401920    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-23T16:05:22.932603Z",
     "start_time": "2021-12-23T16:05:22.920605Z"
    }
   },
   "outputs": [],
   "source": [
    "arg = {'eps': 0.11, 'lam': 0.25, 'rob': 0, 'opt': 'BBB', 'gpu': '-1'}\n",
    "\n",
    "# epsilon: the strength of the adversary\n",
    "eps = arg['eps']\n",
    "# lamada: the probability of seeing the clean data (lambda = 1.0 means no adversarial data considered, lambda = 0.0 means only adversarial data considered)\n",
    "lam = arg['lam'] \n",
    "# robustness mode, 0:标准 1:IBP 2:PGD\n",
    "rob = arg['rob'] \n",
    "# the variational inference method\n",
    "optim = arg['opt']\n",
    "gpu = arg['gpu']\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-23T16:05:23.339602Z",
     "start_time": "2021-12-23T16:05:23.325599Z"
    }
   },
   "outputs": [],
   "source": [
    "inf = 10\n",
    "learning_rate = 0.45; decay=0.0\n",
    "\n",
    "if (rob == 0):\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "elif (rob != 0):\n",
    "    loss = BayesKeras.optimizers.losses.robust_crossentropy_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-23T16:05:23.999690Z",
     "start_time": "2021-12-23T16:05:23.981688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This optimizer does not have a default compilation method. Please make sure to call the correct .compile method before use.\n"
     ]
    }
   ],
   "source": [
    "opt = BayesByBackprop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编译模型\n",
    "\n",
    "- 生成先验、后验分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-23T16:05:25.328047Z",
     "start_time": "2021-12-23T16:05:24.879516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================进入prior_generator===============================\n",
      "BayesKeras: Using implicit prior\n",
      "(784, 512) 0.03571428571428571\n",
      "(512, 10) 0.04419417382415922\n",
      "BayesKeras: Using implicit prior\n",
      "(784, 512) 0.03571428571428571\n",
      "(512, 10) 0.04419417382415922\n",
      "===============================退出prior_generator===============================\n",
      "计算后验方差\n",
      "BayesKeras: Using passed loss_fn as the data likelihood in the KL loss\n"
     ]
    }
   ],
   "source": [
    "bayes_model = opt.compile(model, loss_fn=loss, learning_rate=learning_rate, epochs=1, # epochs=20, \n",
    "                          batch_size=128, linear_schedule=True,\n",
    "                          decay=decay, robust_train=rob, c=inf,\n",
    "                          burn_in=3, steps=25, b_steps=20, epsilon=eps, rob_lam=lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T09:42:44.240806Z",
     "start_time": "2021-12-22T09:42:44.172911Z"
    }
   },
   "source": [
    "## 训练模型\n",
    "\n",
    "循环epochs次\n",
    "\n",
    "- 更新学习率\n",
    "\n",
    "- 在训练集上用BBB方法进行采样，获取后验和后验方差\n",
    "\n",
    "- 在测试集上验证模型\n",
    "\n",
    "- 复位损失、准确率状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-23T16:07:08.304758Z",
     "start_time": "2021-12-23T16:05:26.389996Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 469/469 [01:40<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 7.753, acc: 0.836, val_loss: 0.213, val_acc: 0.932\n",
      "Running time: 0:01:41.897537\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "bayes_model.train(X_train, y_train, X_test, y_test)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print('Running time: %s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-23T16:07:09.329764Z",
     "start_time": "2021-12-23T16:07:08.556240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\T470\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型BBB_FCN_Posterior_0_test2保存成功\n"
     ]
    }
   ],
   "source": [
    "# Save your approxiate Bayesian posterior\n",
    "bayes_model.save(\"%s_FCN_Posterior_%s_test2\" % (optim, rob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-23T05:53:14.168150Z",
     "start_time": "2021-12-23T05:50:56.385Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(100).batch(\n",
    "            self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CertifiableBayesianInference",
   "language": "python",
   "name": "certifiablebayesianinference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}