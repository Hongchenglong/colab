{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T08:14:33.767812Z",
     "start_time": "2021-12-22T08:14:33.757812Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path(os.getcwd())\n",
    "sys.path.append(str(path.parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T08:14:54.982808Z",
     "start_time": "2021-12-22T08:14:34.188811Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\T470\\Anaconda3\\envs\\CertifiableBayesianInference\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "# Impliments the BayesByBackprop optimizer for BayesKeras\n",
    "\n",
    "import os\n",
    "import math\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "from BayesKeras.optimizers import optimizer\n",
    "from BayesKeras.optimizers import losses\n",
    "from BayesKeras import analyzers\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# A dumb mistake on my part which needs to be factored out\n",
    "\n",
    "\n",
    "def softplus(x):\n",
    "    return tf.math.softplus(x)\n",
    "\n",
    "\n",
    "class BayesByBackprop():\n",
    "    def __init__(self):\n",
    "        print(\"This optimizer does not have a default compilation method. Please make sure to call the correct .compile method before use.\")\n",
    "\n",
    "    # I set default params for each sub-optimizer but none for the super class for pretty obvious reasons\n",
    "    def compile(self, keras_model, loss_fn, batch_size=64, learning_rate=0.15, decay=0.0,\n",
    "                epochs=10, prior_mean=-1, prior_var=-1, **kwargs):\n",
    "        ###############################super().compile()###############################\n",
    "        self.model = keras_model\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.epochs = epochs\n",
    "        self.loss_func = loss_fn\n",
    "\n",
    "        self.log_dir = kwargs.get('log_file', '/tmp/BayesKeras.log')\n",
    "\n",
    "        self.det = kwargs.get('deterministic', False)\n",
    "        self.inflate_prior = kwargs.get('inflate_prior', 1)\n",
    "        self.input_noise = kwargs.get('input_noise', 0.0)\n",
    "        # 先验、后验都调用prior_generator\n",
    "        print(\"===============================进入prior_generator===============================\")\n",
    "        self.prior_mean, self.prior_var = self.prior_generator(\n",
    "            prior_mean, prior_var)\n",
    "        self.posterior_mean, self.posterior_var = self.prior_generator(\n",
    "            prior_mean, prior_var)\n",
    "        print(\"===============================退出prior_generator===============================\")\n",
    "\n",
    "        self.train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "        self.valid_loss = tf.keras.metrics.Mean(name=\"valid_loss\")\n",
    "\n",
    "        # Right now I only have one accessory metric. Will come back and add many later.\n",
    "        self.train_metric = kwargs.get(\n",
    "            'metric', tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_acc\"))\n",
    "        self.valid_metric = kwargs.get(\n",
    "            'metric', tf.keras.metrics.SparseCategoricalAccuracy(name=\"valid_acc\"))\n",
    "        self.extra_metric = kwargs.get(\n",
    "            'metric', tf.keras.metrics.SparseCategoricalAccuracy(name=\"extra_acc\"))\n",
    "\n",
    "        self.robust_train = kwargs.get('robust_train', 0)\n",
    "        if(self.robust_train != 0):\n",
    "            print(\"BayesKeras: Detected robust training at compilation. Please ensure you have selected a robust-compatible loss\")\n",
    "            self.epochs += 1\n",
    "        self.epsilon = kwargs.get('epsilon', 0.1)\n",
    "        self.robust_lambda = kwargs.get('rob_lam', 0.5)\n",
    "        self.robust_linear = kwargs.get('linear_schedule', True)\n",
    "\n",
    "        self.attack_loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "        self.loss_monte_carlo = kwargs.get('loss_mc', 2)\n",
    "        self.eps_dist = tfp.distributions.Exponential(\n",
    "            rate=1.0/float(self.epsilon))\n",
    "\n",
    "        self.acc_log = []\n",
    "        self.rob_log = []\n",
    "        self.loss_log = []\n",
    "        ###############################super().compile()###############################\n",
    "\n",
    "        # Now we get into the BayesByBackprop specific enrichments to the class\n",
    "        # Post process our variances to all be stds:\n",
    "        print(\"计算后验方差\")\n",
    "        for i in range(len(self.posterior_var)):\n",
    "            self.posterior_var[i] = tf.math.log(\n",
    "                tf.math.exp(self.posterior_var[i])-1)\n",
    "        self.kl_weight = kwargs.get('kl_weight', 1.0)\n",
    "        self.kl_component = tf.keras.metrics.Mean(name=\"kl_comp\")\n",
    "        print(\"BayesKeras: Using passed loss_fn as the data likelihood in the KL loss\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def train(self, X_train, y_train, X_test=None, y_test=None):\n",
    "        ###############################super().train()###############################\n",
    "        self.N = len(X_train)\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(\n",
    "            100).batch(self.batch_size)  # from_tensor_slices: 切分传入Tensor的第一个维度，生成相应的dataset\n",
    "        test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "            (X_test, y_test)).batch(self.batch_size)\n",
    "\n",
    "        if(self.robust_linear):\n",
    "            self.max_eps = self.epsilon\n",
    "            self.epsilon = 0.0\n",
    "            self.max_robust_lambda = self.robust_lambda\n",
    "\n",
    "        lr = self.learning_rate\n",
    "        decay = self.decay\n",
    "        for epoch in range(self.epochs):\n",
    "            lrate = self.learning_rate * (1 / (1 + self.decay * epoch))\n",
    "\n",
    "            # Run the model through train and test sets respectively\n",
    "            print(\"===============================开始BBB初始采样===============================\")\n",
    "            for (features, labels) in tqdm(train_ds):\n",
    "                features += np.random.normal(loc=0.0, scale=self.input_noise, size=features.shape)\n",
    "                self.posterior, self.posterior_var = self.step(\n",
    "                    features, labels, lrate)\n",
    "            print(\"===============================结束BBB初始采样===============================\")\n",
    "            print(\"===============================开始model_validate===============================\")\n",
    "            for test_features, test_labels in test_ds:\n",
    "                self.model_validate(test_features, test_labels)\n",
    "            print(\"===============================结束model_validate===============================\")\n",
    "\n",
    "            # Grab the results\n",
    "            (loss, acc) = self.train_loss.result(), self.train_metric.result()\n",
    "            (val_loss, val_acc) = self.valid_loss.result(), self.valid_metric.result()\n",
    "            self.logging(loss, acc, val_loss, val_acc, epoch)\n",
    "\n",
    "            # Clear the current state of the metrics\n",
    "            self.train_loss.reset_states(), self.train_metric.reset_states()\n",
    "            self.valid_loss.reset_states(), self.valid_metric.reset_states()\n",
    "            self.extra_metric.reset_states()\n",
    "\n",
    "            if(self.robust_linear):\n",
    "                self.epsilon += self.max_eps/self.epochs\n",
    "        ###############################super().train()###############################\n",
    "\n",
    "    def step(self, features, labels, lrate):\n",
    "        \"\"\"\n",
    "        Initial sampling for BBB\n",
    "        \"\"\"\n",
    "        init_weights = []\n",
    "        noise_used = []\n",
    "        for i in range(len(self.posterior_mean)):\n",
    "            noise = tf.random.normal(shape=self.posterior_var[i].shape,\n",
    "                                     mean=tf.zeros(self.posterior_var[i].shape), stddev=1.0)\n",
    "            var_add = tf.multiply(softplus(self.posterior_var[i]), noise)\n",
    "            #var_add = tf.multiply(self.posterior_mean[i], noise)\n",
    "            w = tf.math.add(self.posterior_mean[i], var_add)\n",
    "            noise_used.append(noise)\n",
    "            init_weights.append(w)\n",
    "        self.model.set_weights(init_weights)\n",
    "\n",
    "        # Define the GradientTape context\n",
    "        # Below we add an extra variable for IBP\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(self.posterior_mean)\n",
    "            tape.watch(self.posterior_var)  # tape.watch(init_weights)\n",
    "            predictions = self.model(features)\n",
    "\n",
    "            if(self.robust_train == 0):\n",
    "                worst_case = predictions  # cheap hack lol\n",
    "                loss, kl_comp = losses.KL_Loss(labels, predictions, self.model.trainable_variables,\n",
    "                                               self.prior_mean, self.prior_var,\n",
    "                                               self.posterior_mean, self.posterior_var,\n",
    "                                               self.loss_func, self.kl_weight)\n",
    "            elif(int(self.robust_train) == 1):\n",
    "                # Get the probabilities\n",
    "                logit_l, logit_u = analyzers.IBP(\n",
    "                    self, features, self.model.trainable_variables, eps=self.epsilon)\n",
    "                #!*! TODO: Undo the hardcoding of depth in this function\n",
    "                v1 = tf.one_hot(labels, depth=10)\n",
    "                v2 = 1 - tf.one_hot(labels, depth=10)\n",
    "                worst_case = tf.math.add(tf.math.multiply(\n",
    "                    v2, logit_u), tf.math.multiply(v1, logit_l))\n",
    "\n",
    "                # Now we have the worst case softmax probabilities\n",
    "                worst_case = self.model.layers[-1].activation(worst_case)\n",
    "                # Calculate the loss\n",
    "                loss, kl_comp = losses.robust_KL_Loss(labels, predictions, self.model.trainable_variables,\n",
    "                                                      self.prior_mean, self.prior_var,\n",
    "                                                      self.posterior_mean, self.posterior_var,\n",
    "                                                      self.loss_func, self.kl_weight,\n",
    "                                                      worst_case, self.robust_lambda)\n",
    "            elif(int(self.robust_train) == 2):\n",
    "                features_adv = analyzers.PGD(\n",
    "                    self, features, self.attack_loss, eps=self.epsilon, num_models=-1)\n",
    "                # Get the probabilities\n",
    "                worst_case = self.model(features_adv)\n",
    "                # print(predictions[0], worst_case[0])\n",
    "                # Calculate the loss\n",
    "                loss, kl_comp = losses.robust_KL_Loss(labels, predictions, self.model.trainable_variables,\n",
    "                                                      self.prior_mean, self.prior_var,\n",
    "                                                      self.posterior_mean, self.posterior_var,\n",
    "                                                      self.loss_func, self.kl_weight,\n",
    "                                                      worst_case, self.robust_lambda)\n",
    "        # Get the gradients\n",
    "        weight_gradient = tape.gradient(loss, self.model.trainable_variables)\n",
    "        mean_gradient = tape.gradient(loss, self.posterior_mean)\n",
    "        var_gradient = tape.gradient(loss, self.posterior_var)\n",
    "        #init_gradient = tape.gradient(loss, init_weights)\n",
    "\n",
    "        posti_mean_grad = []\n",
    "        posti_var_grad = []\n",
    "        # !*! - Make the weight and init gradients the same variable and retest\n",
    "        for i in range(len(mean_gradient)):\n",
    "            #weight_gradient[i] = tf.math.add(weight_gradient[i], init_gradient[i])\n",
    "            weight_gradient[i] = tf.cast(weight_gradient[i], 'float32')\n",
    "            mean_gradient[i] = tf.cast(mean_gradient[i], 'float32')\n",
    "            f = tf.math.add(weight_gradient[i], mean_gradient[i])\n",
    "            posti_mean_grad.append(f)\n",
    "            v = tf.math.divide(\n",
    "                noise_used[i], 1+tf.math.exp(tf.math.multiply(self.posterior_var[i], -1)))\n",
    "            v = tf.math.multiply(v, weight_gradient[i])\n",
    "            v = tf.math.add(v, var_gradient[i])\n",
    "            posti_var_grad.append(v)\n",
    "        #gradients = posti_mean_grad\n",
    "\n",
    "        # APPLICATION OF WEIGHTS\n",
    "        new_posti_var = []\n",
    "        new_posti_mean = []\n",
    "        for i in range(len(mean_gradient)):\n",
    "            pdv = tf.math.multiply(posti_var_grad[i], lrate)\n",
    "            pdm = tf.math.multiply(posti_mean_grad[i], lrate)\n",
    "            v = tf.math.subtract(self.posterior_var[i], pdv)\n",
    "            m = tf.math.subtract(self.posterior_mean[i], pdm)\n",
    "            new_posti_var.append(v)\n",
    "            new_posti_mean.append(m)\n",
    "\n",
    "        self.train_loss(loss)\n",
    "        self.train_metric(labels, predictions)\n",
    "        #self.train_rob(labels, worst_case)\n",
    "        self.kl_component(kl_comp)\n",
    "        self.posterior_mean = new_posti_mean\n",
    "        self.posterior_var = new_posti_var\n",
    "        return new_posti_mean, new_posti_var\n",
    "\n",
    "    def prior_generator(self, means, vars):\n",
    "        if(type(means) == int and type(vars) == int):\n",
    "            if(means < 0 or vars < 0):\n",
    "                model_mean, model_var = self._gen_implicit_prior()\n",
    "                # for i in range(len(model_var)):\n",
    "                #    model_var[i] = tf.math.log(tf.math.exp(self.model_var[i])-1)\n",
    "                return model_mean, model_var\n",
    "        if(type(means) == int or type(means) == float):\n",
    "            if(means == -1):\n",
    "                means = 0.0\n",
    "            mean_params = [means for i in range(len(self.model.weights))]\n",
    "            means = mean_params\n",
    "        if(type(vars) == int or type(vars) == float):\n",
    "            if(vars == -1):\n",
    "                vars = 0.0\n",
    "            var_params = [vars for i in range(len(self.model.weights))]\n",
    "            vars = var_params\n",
    "        model_mean = []\n",
    "        model_var = []\n",
    "        index = 0.0\n",
    "        for weight in self.model.weights:\n",
    "            param_index = math.floor(index/2.0)\n",
    "            mean_i = tf.math.multiply(\n",
    "                tf.ones(weight.shape), means[param_index])\n",
    "            vari_i = tf.math.multiply(tf.ones(weight.shape), vars[param_index])\n",
    "            model_mean.append(mean_i)\n",
    "            model_var.append(vari_i)\n",
    "            index += 1\n",
    "        return model_mean, model_var\n",
    "\n",
    "    \"\"\"\n",
    "    根据模型每一层的形状生成权重w和偏置b的先验均值分布\n",
    "    权重w：0\n",
    "    偏置b：std = math.sqrt(self.inflate_prior/(nin)) \n",
    "    \"\"\"\n",
    "    def _gen_implicit_prior(self):\n",
    "        print(\"BayesKeras: Using implicit prior\")\n",
    "        prior_mean = []\n",
    "        prior_var = []\n",
    "        for i in range(len(self.model.layers)):\n",
    "            try:\n",
    "                sha = self.model.layers[i].get_weights()[0].shape\n",
    "                b_sha = self.model.layers[i].get_weights()[1].shape\n",
    "                if(len(sha) > 2):\n",
    "                    nin = 1\n",
    "                    for i in range(len(sha)-1):\n",
    "                        nin *= sha[i]\n",
    "                else:\n",
    "                    nin = sha[0]\n",
    "                # 标准差\n",
    "                std = math.sqrt(self.inflate_prior/(nin))\n",
    "                print(sha, std)\n",
    "                mean_w = tf.zeros(sha)\n",
    "                var_w = tf.ones(sha) * std\n",
    "                mean_b = tf.zeros(b_sha)\n",
    "                var_b = tf.ones(b_sha) * std\n",
    "                # 分别加入权重w和偏置b的先验均值分布\n",
    "                prior_mean.append(mean_w)\n",
    "                prior_mean.append(mean_b)\n",
    "                prior_var.append(var_w)\n",
    "                prior_var.append(var_b)\n",
    "            except:\n",
    "                pass\n",
    "        return prior_mean, prior_var\n",
    "\n",
    "    def model_validate(self, features, labels):\n",
    "        # self.model.set_weights(self.sample())\n",
    "        predictions = self.model(features)\n",
    "        if(self.robust_train == 1):  # We only check with IBP if we need to\n",
    "            logit_l, logit_u = analyzers.IBP(\n",
    "                self, features, self.model.get_weights(), self.epsilon)\n",
    "            #logit_l, logit_u = analyzers.IBP(self, features, self.model.trainable_variables, 0.0)\n",
    "            v1 = tf.one_hot(labels, depth=10)\n",
    "            v2 = 1 - tf.one_hot(labels, depth=10)\n",
    "            worst_case = tf.math.add(tf.math.multiply(\n",
    "                v2, logit_u), tf.math.multiply(v1, logit_l))\n",
    "            worst_case = self.model.layers[-1].activation(worst_case)\n",
    "            v_loss = self.loss_func(\n",
    "                labels, predictions, worst_case, self.robust_lambda)\n",
    "            self.extra_metric(labels, worst_case)\n",
    "        elif(self.robust_train == 2):\n",
    "            v_loss = self.loss_func(\n",
    "                labels, predictions, predictions, self.robust_lambda)\n",
    "            worst_case = predictions\n",
    "        elif(self.robust_train == 3 or self.robust_train == 5):  # We only check with IBP if we need to\n",
    "            logit_l, logit_u = analyzers.IBP(\n",
    "                self, features, self.model.get_weights(), self.epsilon)\n",
    "            #logit_l, logit_u = analyzers.IBP(self, features, self.model.trainable_variables, 0.0)\n",
    "            # print(logit_l.shape)\n",
    "            v1 = tf.squeeze(tf.one_hot(labels, depth=10))\n",
    "            v2 = tf.squeeze(1 - tf.one_hot(labels, depth=10))\n",
    "            #print(v1.shape, v2.shape)\n",
    "            worst_case = tf.math.add(tf.math.multiply(\n",
    "                v2, logit_u), tf.math.multiply(v1, logit_l))\n",
    "            # print(worst_case.shape)\n",
    "            worst_case = self.model.layers[-1].activation(worst_case)\n",
    "            # print(worst_case.shape)\n",
    "            v_loss = self.loss_func(labels, worst_case)\n",
    "            self.extra_metric(labels, worst_case)\n",
    "        else:\n",
    "            v_loss = self.loss_func(labels, predictions)\n",
    "            worst_case = predictions\n",
    "        self.valid_metric(labels, predictions)\n",
    "        self.valid_loss(v_loss)\n",
    "        #self.valid_rob(labels, worst_case)\n",
    "\n",
    "    def logging(self, loss, acc, val_loss, val_acc, epoch):\n",
    "        # Local logging\n",
    "        if(self.robust_train == 0):\n",
    "            template = \"Epoch {}, loss: {:.3f}, acc: {:.3f}, val_loss: {:.3f}, val_acc: {:.3f}\"\n",
    "            print(template.format(epoch+1, loss,\n",
    "                                  acc,\n",
    "                                  val_loss,\n",
    "                                  val_acc))\n",
    "        else:\n",
    "            rob = self.extra_metric.result()\n",
    "            template = \"Epoch {}, loss: {:.3f}, acc: {:.3f}, val_loss: {:.3f}, val_acc: {:.3f}, rob: {:.3f}, (eps = {:.6f})\"\n",
    "            print(template.format(epoch+1, loss,\n",
    "                                  acc,\n",
    "                                  val_loss,\n",
    "                                  val_acc, rob, self.epsilon))\n",
    "        log_template = \"Epoch: {}, Train: [Loss: {:.3f}, Acc: {:.3f}], Test: [Loss: {:.3f}, Acc: {:.3f}]\"\n",
    "        logging.basicConfig(filename=self.log_dir, level=logging.DEBUG)\n",
    "        logging.info(log_template.format(\n",
    "            epoch+1, loss, acc, val_loss, val_acc))\n",
    "\n",
    "    def save(self, path):\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        var = []\n",
    "        for i in range(len(self.posterior_var)):\n",
    "            var.append(softplus(self.posterior_var[i]))\n",
    "        np.save(path+\"/mean\", np.asarray(self.posterior_mean))\n",
    "        np.save(path+\"/var\", np.asarray(var))\n",
    "        self.model.save(path+'/model.h5')\n",
    "        model_json = self.model.to_json()\n",
    "        with open(path+\"/arch.json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        print(\"模型%s保存成功\" % path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T08:14:55.090813Z",
     "start_time": "2021-12-22T08:14:55.080810Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T08:14:56.628807Z",
     "start_time": "2021-12-22T08:14:55.204810Z"
    }
   },
   "outputs": [],
   "source": [
    "# 划分训练集，测试集\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train/255.\n",
    "X_test = X_test/255.\n",
    "X_train = X_train.astype(\"float32\").reshape(-1, 28*28)\n",
    "X_test = X_test.astype(\"float32\").reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T08:14:58.394809Z",
     "start_time": "2021-12-22T08:14:56.760811Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, None, 512)         401920    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, None, 10)          5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 构建Sequential模型\n",
    "model = Sequential()\n",
    "# 构建Dense隐藏层并添加到模型中\n",
    "# 添加具有512个神经元的Dense隐藏层，使用relu激活函数\n",
    "model.add(Dense(512, activation=\"relu\", input_shape=(None, 28*28)))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T08:14:58.599808Z",
     "start_time": "2021-12-22T08:14:58.571813Z"
    }
   },
   "outputs": [],
   "source": [
    "dict = {'eps': 0.11, 'lam': 0.25, 'rob': 0, 'opt': 'BBB', 'gpu': '-1'}\n",
    "\n",
    "# epsilon: the strength of the adversary\n",
    "eps = dict['eps']\n",
    "# lamada: the probability of seeing the clean data (lambda = 1.0 means no adversarial data considered, lambda = 0.0 means only adversarial data considered)\n",
    "lam = dict['lam'] \n",
    "# robustness mode, 0:标准 1:IBP 2:PGD\n",
    "rob = dict['rob'] \n",
    "# the variational inference method\n",
    "optim = dict['opt']\n",
    "gpu = dict['gpu']\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T08:14:58.788809Z",
     "start_time": "2021-12-22T08:14:58.776816Z"
    }
   },
   "outputs": [],
   "source": [
    "inf = 10\n",
    "learning_rate = 0.45; decay=0.0\n",
    "\n",
    "if (rob == 0):\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "elif (rob != 0):\n",
    "    loss = BayesKeras.optimizers.losses.robust_crossentropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T08:14:59.037811Z",
     "start_time": "2021-12-22T08:14:59.017813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This optimizer does not have a default compilation method. Please make sure to call the correct .compile method before use.\n"
     ]
    }
   ],
   "source": [
    "opt = BayesByBackprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T08:14:59.319810Z",
     "start_time": "2021-12-22T08:14:59.168814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################进入prior_generator###############################\n",
      "BayesKeras: Using implicit prior\n",
      "(784, 512) 0.03571428571428571\n",
      "(512, 10) 0.04419417382415922\n",
      "BayesKeras: Using implicit prior\n",
      "(784, 512) 0.03571428571428571\n",
      "(512, 10) 0.04419417382415922\n",
      "###############################退出prior_generator###############################\n",
      "计算后验方差\n",
      "BayesKeras: Using passed loss_fn as the data likelihood in the KL loss\n"
     ]
    }
   ],
   "source": [
    "bayes_model = opt.compile(model, loss_fn=loss, learning_rate=learning_rate, epochs=20, # epochs=20, \n",
    "                          batch_size=128, linear_schedule=True,\n",
    "                          decay=decay, robust_train=rob, c=inf,\n",
    "                          burn_in=3, steps=25, b_steps=20, epsilon=eps, rob_lam=lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T09:20:10.464149Z",
     "start_time": "2021-12-22T08:14:59.465812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################开始BBB初始采样###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 469/469 [03:16<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################结束BBB初始采样###############################\n",
      "###############################开始model_validate###############################\n",
      "###############################结束model_validate###############################\n",
      "Epoch 1, loss: 7.757, acc: 0.838, val_loss: 0.232, val_acc: 0.926\n",
      "###############################开始BBB初始采样###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 469/469 [03:05<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################结束BBB初始采样###############################\n",
      "###############################开始model_validate###############################\n",
      "###############################结束model_validate###############################\n",
      "Epoch 2, loss: 6.876, acc: 0.940, val_loss: 0.157, val_acc: 0.949\n",
      "###############################开始BBB初始采样###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 469/469 [02:24<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################结束BBB初始采样###############################\n",
      "###############################开始model_validate###############################\n",
      "###############################结束model_validate###############################\n",
      "Epoch 3, loss: 6.419, acc: 0.954, val_loss: 0.153, val_acc: 0.953\n",
      "###############################开始BBB初始采样###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 469/469 [02:57<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################结束BBB初始采样###############################\n",
      "###############################开始model_validate###############################\n",
      "###############################结束model_validate###############################\n",
      "Epoch 4, loss: 6.005, acc: 0.961, val_loss: 0.118, val_acc: 0.963\n",
      "###############################开始BBB初始采样###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 469/469 [03:04<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################结束BBB初始采样###############################\n",
      "###############################开始model_validate###############################\n",
      "###############################结束model_validate###############################\n",
      "Epoch 5, loss: 5.644, acc: 0.966, val_loss: 0.128, val_acc: 0.960\n",
      "###############################开始BBB初始采样###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 469/469 [02:51<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################结束BBB初始采样###############################\n",
      "###############################开始model_validate###############################\n",
      "###############################结束model_validate###############################\n",
      "Epoch 6, loss: 5.323, acc: 0.969, val_loss: 0.119, val_acc: 0.963\n",
      "###############################开始BBB初始采样###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 469/469 [03:25<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################结束BBB初始采样###############################\n",
      "###############################开始model_validate###############################\n",
      "###############################结束model_validate###############################\n",
      "Epoch 7, loss: 5.063, acc: 0.971, val_loss: 0.112, val_acc: 0.965\n",
      "###############################开始BBB初始采样###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 469/469 [03:09<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################结束BBB初始采样###############################\n",
      "###############################开始model_validate###############################\n",
      "###############################结束model_validate###############################\n",
      "Epoch 8, loss: 4.869, acc: 0.971, val_loss: 0.135, val_acc: 0.959\n",
      "###############################开始BBB初始采样###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 469/469 [03:16<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################结束BBB初始采样###############################\n",
      "###############################开始model_validate###############################\n",
      "###############################结束model_validate###############################\n",
      "Epoch 9, loss: 4.732, acc: 0.972, val_loss: 0.141, val_acc: 0.957\n",
      "###############################开始BBB初始采样###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 469/469 [03:06<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################结束BBB初始采样###############################\n",
      "###############################开始model_validate###############################\n",
      "###############################结束model_validate###############################\n",
      "Epoch 10, loss: 4.629, acc: 0.973, val_loss: 0.142, val_acc: 0.957\n",
      "###############################开始BBB初始采样###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 469/469 [03:16<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################结束BBB初始采样###############################\n",
      "###############################开始model_validate###############################\n",
      "###############################结束model_validate###############################\n",
      "Epoch 11, loss: 4.552, acc: 0.974, val_loss: 0.114, val_acc: 0.965\n",
      "###############################开始BBB初始采样###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 469/469 [04:55<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################结束BBB初始采样###############################\n",
      "###############################开始model_validate###############################\n",
      "###############################结束model_validate###############################\n",
      "Epoch 12, loss: 4.495, acc: 0.974, val_loss: 0.103, val_acc: 0.971\n",
      "###############################开始BBB初始采样###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 469/469 [03:04<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################结束BBB初始采样###############################\n",
      "###############################开始model_validate###############################\n",
      "###############################结束model_validate###############################\n",
      "Epoch 13, loss: 4.456, acc: 0.974, val_loss: 0.151, val_acc: 0.954\n",
      "###############################开始BBB初始采样###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 469/469 [03:07<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################结束BBB初始采样###############################\n",
      "###############################开始model_validate###############################\n",
      "###############################结束model_validate###############################\n",
      "Epoch 14, loss: 4.407, acc: 0.975, val_loss: 0.115, val_acc: 0.968\n",
      "###############################开始BBB初始采样###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 469/469 [03:22<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################结束BBB初始采样###############################\n",
      "###############################开始model_validate###############################\n",
      "###############################结束model_validate###############################\n",
      "Epoch 15, loss: 4.363, acc: 0.975, val_loss: 0.153, val_acc: 0.956\n",
      "###############################开始BBB初始采样###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 469/469 [03:18<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################结束BBB初始采样###############################\n",
      "###############################开始model_validate###############################\n",
      "###############################结束model_validate###############################\n",
      "Epoch 16, loss: 4.325, acc: 0.976, val_loss: 0.105, val_acc: 0.970\n",
      "###############################开始BBB初始采样###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 469/469 [03:07<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################结束BBB初始采样###############################\n",
      "###############################开始model_validate###############################\n",
      "###############################结束model_validate###############################\n",
      "Epoch 17, loss: 4.284, acc: 0.976, val_loss: 0.112, val_acc: 0.968\n",
      "###############################开始BBB初始采样###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 469/469 [03:06<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################结束BBB初始采样###############################\n",
      "###############################开始model_validate###############################\n",
      "###############################结束model_validate###############################\n",
      "Epoch 18, loss: 4.248, acc: 0.978, val_loss: 0.099, val_acc: 0.971\n",
      "###############################开始BBB初始采样###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 469/469 [03:26<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################结束BBB初始采样###############################\n",
      "###############################开始model_validate###############################\n",
      "###############################结束model_validate###############################\n",
      "Epoch 19, loss: 4.210, acc: 0.978, val_loss: 0.096, val_acc: 0.971\n",
      "###############################开始BBB初始采样###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 469/469 [03:07<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################结束BBB初始采样###############################\n",
      "###############################开始model_validate###############################\n",
      "###############################结束model_validate###############################\n",
      "Epoch 20, loss: 4.169, acc: 0.979, val_loss: 0.137, val_acc: 0.958\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "bayes_model.train(X_train, y_train, X_test, y_test)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print('Running time: %s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T09:20:10.870151Z",
     "start_time": "2021-12-22T09:20:10.642153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\T470\\Anaconda3\\envs\\CertifiableBayesianInference\\lib\\site-packages\\ipykernel_launcher.py:375: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "D:\\Users\\T470\\Anaconda3\\envs\\CertifiableBayesianInference\\lib\\site-packages\\ipykernel_launcher.py:376: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型BBB_FCN_Posterior_0保存成功\n"
     ]
    }
   ],
   "source": [
    "# Save your approxiate Bayesian posterior\n",
    "bayes_model.save(\"%s_FCN_Posterior_%s\" % (optim, rob))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CertifiableBayesianInference",
   "language": "python",
   "name": "certifiablebayesianinference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
